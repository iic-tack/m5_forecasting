{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "slack_notify=False\n",
    "\n",
    "if slack_notify:\n",
    "    import sys\n",
    "    sys.path.append(\"../\")\n",
    "    import slackweb\n",
    "    from slack_url import slack_url\n",
    "    slack = slackweb.Slack(url=slack_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "data_ex = pd.read_csv('./data/backup/data_ex2_light.csv')\n",
    "train = data_ex[data_ex[\"date_id\"] <= 1913]\n",
    "test = data_ex[(1914 <= data_ex[\"date_id\"]) & (data_ex[\"date_id\"] <= 1941)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ex[(1548 <= data_ex[\"date_id\"]) & (data_ex[\"date_id\"] <= 1941)].to_csv('./data/backup/data_ex2_light.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.052784092 GB\n",
      "4.781747627999999 GB\n",
      "0.366881216 GB\n",
      "1.5999999999999998e-08 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sys.getsizeof(data_ex)/1000/1000/1000} GB\")\n",
    "print(f\"{sys.getsizeof(train)/1000/1000/1000} GB\")\n",
    "print(f\"{sys.getsizeof(test)/1000/1000/1000} GB\")\n",
    "print(f\"{sys.getsizeof(subm)/1000/1000/1000} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.133087609999997 GB\n",
      "19.781208258000003 GB\n",
      "0.366881216 GB\n",
      "0.366881216 GB\n"
     ]
    }
   ],
   "source": [
    "# data_ex2.csv\n",
    "# print(f\"{sys.getsizeof(data_ex)/1000/1000/1000} GB\")\n",
    "# print(f\"{sys.getsizeof(train)/1000/1000/1000} GB\")\n",
    "# print(f\"{sys.getsizeof(test)/1000/1000/1000} GB\")\n",
    "# print(f\"{sys.getsizeof(subm)/1000/1000/1000} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features and params \n",
    "features = [\n",
    "    'item_id', \n",
    "    'date_id', \n",
    "    'date', \n",
    "    'wm_yr_wk', \n",
    "    'wday', \n",
    "    'month', \n",
    "    'year', \n",
    "    'event_name_1', \n",
    "    'event_name_2', \n",
    "    'snap_CA', \n",
    "    'snap_TX', \n",
    "    'snap_WI', \n",
    "    'sell_price', \n",
    "    'dept_id_FOODS_1', \n",
    "    'dept_id_FOODS_2', \n",
    "    'dept_id_FOODS_3', \n",
    "    'dept_id_HOBBIES_1', \n",
    "    'dept_id_HOBBIES_2', \n",
    "    'dept_id_HOUSEHOLD_1', \n",
    "    'dept_id_HOUSEHOLD_2', \n",
    "    'cat_id_FOODS', \n",
    "    'cat_id_HOBBIES', \n",
    "    'cat_id_HOUSEHOLD', \n",
    "    'store_id_CA_1', \n",
    "    'store_id_CA_2', \n",
    "    'store_id_CA_3', \n",
    "    'store_id_CA_4', \n",
    "    'store_id_TX_1', \n",
    "    'store_id_TX_2', \n",
    "    'store_id_TX_3', \n",
    "    'store_id_WI_1', \n",
    "    'store_id_WI_2', \n",
    "    'store_id_WI_3', \n",
    "    'state_id_CA', \n",
    "    'state_id_TX', \n",
    "    'state_id_WI', \n",
    "    'event_type_1_Cultural', \n",
    "    'event_type_1_National', \n",
    "    'event_type_1_Religious', \n",
    "    'event_type_1_Sporting', \n",
    "    'event_type_1_nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'rmse',\n",
    "    'objective': 'regression',\n",
    "    'n_jobs': -1,\n",
    "    'seed': 236,\n",
    "    'learning_rate': 0.1,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 10, \n",
    "    'colsample_bytree': 0.75,\n",
    "    'lambda_l1': 0.0,\n",
    "    'lambda_l2': 1.0,\n",
    "    'min_data_in_leaf': 1,\n",
    "    'max_depth': 5,\n",
    "}\n",
    "\n",
    "# params = {\n",
    "#     #'boosting_type': 'gbdt',\n",
    "#     #'metric': 'rmse',\n",
    "#     #'objective': 'regression',\n",
    "#     #'learning_rate': 0.1,\n",
    "#     'gamma': 0.0,\n",
    "#     'lambda_l1': 0.0,\n",
    "#     'lambda_l2': 1.0,\n",
    "#     'min_data_in_leaf': 1,\n",
    "#     'max_depth': 5,\n",
    "#     #'bagging_fraction': 0.8,\n",
    "#     #'colsample_bytree': 0.8,\n",
    "#     #'seed': 71,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "train_set = lgb.Dataset(train[features], train['num'], free_raw_data=False)\n",
    "test_set = lgb.Dataset(test[features], test['num'], free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train\n",
    "def score(params):\n",
    "    for h in params:\n",
    "        print(h)\n",
    "    params[\"min_data_in_leaf\"] = int(params[\"min_data_in_leaf\"])\n",
    "    params[\"max_depth\"] = int(params[\"max_depth\"])\n",
    "\n",
    "    model = lgb.train(params,\n",
    "                    train_set, \n",
    "                    num_boost_round = 10000,\n",
    "                    early_stopping_rounds = 50,\n",
    "                    valid_sets = [train_set, test_set],\n",
    "                    valid_names=[\"train\", \"test\"], \n",
    "                    verbose_eval = 100)\n",
    "                    \n",
    "    pred = model.predict(test[features])\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred, test[\"num\"]))\n",
    "    history.append((params, score))\n",
    "    return {\"loss\":score, \"status\":STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore range for params\n",
    "param_space = {\n",
    "    'min_data_in_leaf': hp.quniform('min_data_in_leaf', 0, 100, 10),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "    'bagging_fraction': hp.quniform('bagging_fraction', 0.6, 0.95, 0.05),\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
    "    'metrics': 'rmse',\n",
    "    'bagging_freq': 10\n",
    "    }\n",
    "    # 'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "    # 余裕があればlambda_l1, lambda_l2も調整する\n",
    "    # 'lambda_l1' : hp.loguniform('lambda_l1', np.log(1e-8), np.log(1.0)),\n",
    "    # 'lambda_l2' : hp.loguniform('lambda_l2', np.log(1e-6), np.log(10.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params exploring by hyperopt\n",
    "max_evals = 10\n",
    "trials = Trials()\n",
    "history = []\n",
    "fmin(score, space=param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get params and score\n",
    "history = sorted(history, key=lambda tpl:tpl[1])\n",
    "best=history[0]\n",
    "print(f\"best params:{best[0]}, score:{best[1]:.4f}\")\n",
    "if slack_notify:\n",
    "    slack.notify(text=f\"*params_tuning.py has finished!:*\\nbest params:{best[0]}, score:{best[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param_space = {\n",
    "    'min_data_in_leaf': hp.quniform('min_data_in_leaf', 0, 100, 10),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "    'bagging_fraction': hp.quniform('bagging_fraction', 0.6, 0.95, 0.05),\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05)}\n",
    "で\n",
    "{'bagging_fraction': 0.8,\n",
    " 'colsample_bytree': 0.9,\n",
    " 'max_depth': 7.0,\n",
    " 'min_data_in_leaf': 0.0}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9b3f7649d8471c104681edca661db549e4b9e73857a4d2c57862218cae5e35f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('moji': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
